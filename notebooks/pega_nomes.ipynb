{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlKdgE6lri5jVxO2MZaYjr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install bs4\n","!pip install requests\n","from bs4 import BeautifulSoup\n","import requests as requests\n","import re\n","import pandas as pd"],"metadata":{"id":"uTfhkWNfxfw-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHqKveXixOXT"},"outputs":[],"source":["# URL do site\n","url = 'https://www.palmas.to.gov.br'\n","response = requests.get(url)\n","html = response.text\n","\n","soup = BeautifulSoup(html, 'html.parser')\n","\n","links = soup.find_all('a', href=re.compile(r'^(?!.*noticia).*secretarias', flags=re.IGNORECASE))\n","\n","if not links:\n","  links = soup.find_all('a', href=re.compile(r'^(?!.*noticia).*orgaos', flags=re.IGNORECASE))\n","\n","if not links:\n"," links = soup.find_all('a')\n"," links = [link['href'] for link in links if 'secretaria' in link.get_text().lower()]\n","\n","#if not links:\n","\n","for link in links:\n","    link = link[\"href\"]\n","    print(f'Link: {link}')\n"]},{"cell_type":"code","source":["\n","def extract_links_and_names(url):\n","    # Send an HTTP GET request to the URL\n","    response = requests.get(url)\n","\n","    # Get the HTML content of the page\n","    html = response.text\n","\n","    # Create a BeautifulSoup object to parse the HTML\n","    soup = BeautifulSoup(html, 'html.parser')\n","\n","    # Find all <a> tags with an href attribute\n","    links = soup.find_all('a')\n","\n","    # Initialize an empty list to store names\n","    names_list = []\n","\n","    # Iterate through the found links\n","    for link in links:\n","        # Get the href attribute from the link\n","        link_href = link.get('href')\n","\n","        # Check if the link is not None and if it starts with 'http' (to avoid relative links)\n","        if link_href and link_href.startswith('http'):\n","            # Send a request to the linked page\n","            linked_page_response = requests.get(link_href)\n","\n","            # Get the HTML content of the linked page\n","            linked_page_html = linked_page_response.text\n","\n","            # Create a BeautifulSoup object for the linked page\n","            linked_page_soup = BeautifulSoup(linked_page_html, 'html.parser')\n","\n","            # Find names in the linked page (you may need to adjust this based on the actual HTML structure)\n","            names = linked_page_soup.find_all('span', class_='name')  # Example: finding names in span tags with class 'name'\n","\n","            # Extract and append names to the names_list\n","            for name in names:\n","                names_list.append(name.get_text())\n","\n","    return names_list\n","\n","# Example usage:\n","url_to_scrape = 'https://www.lajeado.to.gov.br/pagina/educacao'\n","extracted_names = extract_links_and_names(url_to_scrape)\n","\n","# Print the extracted names\n","print('Extracted Names:')\n","for name in extracted_names:\n","    print(name)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2xO2Yhn0avR","executionInfo":{"status":"ok","timestamp":1701441770562,"user_tz":180,"elapsed":7303,"user":{"displayName":"Joao Pedro Trevisan","userId":"04469965309267366256"}},"outputId":"7ccbf59d-919d-4b4b-e840-11a9ddfbf8c2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracted Names:\n"]}]}]}